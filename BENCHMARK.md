# LXC vs libvirt/KVM Performance Comparison

## Test Environment
- **Device**: Raspberry Pi 5
- **OS**: Debian-based (Raspberry Pi OS)
- **CPU**: ARM64 (4 cores)
- **RAM**: 7.5GB total
- **Network**: Bridged to br0 (10.0.0.0/24)
- **Date**: 2026-02-18

## Test Configuration
Both tests used identical parameters:
- **OS**: Debian
- **CPUs**: 1 (resolved to 4 vCPUs)
- **RAM**: 512MB
- **Disk**: 5GB

## Performance Benchmarks

### 1. Creation Time

| Metric | LXC Container | libvirt/KVM VM | Difference |
|--------|---------------|----------------|------------|
| **Total time** | 50.8s | 2m 6.5s (126.5s) | **+75.7s (2.5x slower)** |
| User time | 19.2s | 4.4s | -14.8s |
| System time | 9.3s | 2.5s | -6.8s |

**Analysis**:
- LXC: ~51 seconds (downloads and extracts Debian rootfs)
- VM: ~127 seconds (uses cached cloud image, but cloud-init first boot is slow)
- **VM is 2.5x slower to create**

### 2. Stop/Shutdown Time

| Metric | LXC Container | libvirt/KVM VM | Difference |
|--------|---------------|----------------|------------|
| **Total time** | 1.375s | 0.322s | **-1.053s (4.3x faster)** |
| User time | 0.187s | 0.199s | +0.012s |
| System time | 0.123s | 0.072s | -0.051s |

**Analysis**:
- LXC: ~1.4 seconds (stops processes and unmounts)
- VM: ~0.3 seconds (sends shutdown signal, doesn't wait)
- **VM command is 4.3x faster** (but actual shutdown takes longer)

### 3. Start Time

| Metric | LXC Container | libvirt/KVM VM | Difference |
|--------|---------------|----------------|------------|
| **Total time** | 0.525s | 1.586s | **+1.061s (3x slower)** |
| User time | 0.178s | 0.188s | +0.010s |
| System time | 0.088s | 0.078s | -0.010s |

**Analysis**:
- LXC: ~0.5 seconds (starts processes)
- VM: ~1.6 seconds (starts QEMU process)
- **VM is 3x slower to start**

### 4. Boot to SSH Ready

| Metric | LXC Container | libvirt/KVM VM | Difference |
|--------|---------------|----------------|------------|
| **Time to SSH** | ~10s (after start) | ~30-40s (after start) | **+20-30s (3-4x slower)** |
| SSH connection | 1.032s | N/A (IP detection issue) | - |

**Analysis**:
- LXC: ~10 seconds from start to SSH ready
- VM: ~30-40 seconds from start to SSH ready
- **VM is 3-4x slower to boot**

### 5. Memory Usage

#### Host Memory
```
Total: 7.5GB
Used: 3.1GB
Free: 102MB
Available: 4.1GB
```

#### LXC Container Memory
```
Total: 7.5GB (shares host memory)
Used: 3.1GB (same as host)
Available: 4.1GB
```
**LXC sees host memory** - no isolation

#### VM Memory
```
Total: 467MB (512MB allocated, ~45MB for kernel)
Used: 58MB
Free: 323MB
Available: 398MB
```
**VM has isolated memory** - true isolation

**Analysis**:
- LXC: Shares host memory, no overhead
- VM: 512MB allocated, ~58MB actually used
- **VM uses ~512MB dedicated RAM vs LXC's shared memory**

### 6. Disk Space Usage

| Metric | LXC Container | libvirt/KVM VM | Difference |
|--------|---------------|----------------|------------|
| **Storage location** | /var/lib/lxc/ | ~/.nox/vms/ | - |
| **Total size** | 492MB | 238MB | **-254MB (VM smaller)** |
| **Root filesystem** | 917GB (host) | 4.8GB (isolated) | - |
| **Used space** | 393GB (host) | 788MB | - |

**Additional VM overhead**:
- Shared cache: 316MB (one-time, in ~/.nox/images/)
- Per-VM: 238MB

**Analysis**:
- LXC: 492MB (full Debian rootfs)
- VM: 238MB (qcow2 disk) + 316MB shared cache
- **VM uses less space per instance** (238MB vs 492MB)
- **First VM costs more** (238MB + 316MB = 554MB)
- **Additional VMs cheaper** (238MB vs 492MB)

## Summary Table

| Metric | LXC | VM | VM Performance |
|--------|-----|-----|----------------|
| **Creation time** | 51s | 127s | üî¥ 2.5x slower |
| **Stop command** | 1.4s | 0.3s | üü¢ 4.3x faster |
| **Start command** | 0.5s | 1.6s | üî¥ 3x slower |
| **Boot to SSH** | 10s | 30-40s | üî¥ 3-4x slower |
| **Memory overhead** | ~0MB | 512MB | üî¥ 512MB per VM |
| **Disk per instance** | 492MB | 238MB | üü¢ 51% smaller |
| **First instance disk** | 492MB | 554MB | üî¥ 13% larger |
| **Memory isolation** | ‚ùå Shared | ‚úÖ Isolated | üü¢ Better |
| **Tailscale support** | ‚ùå Broken | ‚úÖ Works | üü¢ Fixed |

## Performance Loss Analysis

### Time Overhead
- **Creation**: +75.7 seconds (2.5x slower)
- **Boot**: +20-30 seconds (3-4x slower)
- **Start**: +1.1 seconds (3x slower)
- **Stop**: -1.1 seconds (4.3x faster - command only)

### Resource Overhead
- **RAM**: +512MB per VM (dedicated allocation)
- **Disk**: -254MB per VM (more efficient qcow2)
- **First VM disk**: +62MB (includes shared cache)

### What We Gain
‚úÖ **Tailscale works** (original issue - FIXED)
‚úÖ **Full kernel isolation** (better security)
‚úÖ **Memory isolation** (true resource limits)
‚úÖ **Better compatibility** (VPNs, Docker, etc.)
‚úÖ **Smaller disk footprint** per VM (after first)

### What We Lose
üî¥ **2.5x slower creation** (127s vs 51s)
üî¥ **3-4x slower boot** (30-40s vs 10s)
üî¥ **512MB RAM per VM** (vs shared memory)

## Verdict

The performance loss is **acceptable** given the benefits:

1. **Creation time**: 127s is still reasonable for VM creation
2. **Boot time**: 30-40s is acceptable for occasional restarts
3. **RAM overhead**: 512MB is manageable on modern hardware
4. **Disk efficiency**: VMs actually use less disk per instance

The **critical gain** is that Tailscale and other VPN software now work, which was the primary reason for the migration.

## Optimization Opportunities

To reduce performance gap:
1. **Pre-seed cloud images** during installation
2. **Disable cloud-init reboot** (save ~10-15s on creation)
3. **Use Alpine** instead of Debian (~150MB RAM vs 512MB)
4. **Optimize cloud-init** (disable unnecessary modules)
5. **Use VM snapshots** for faster cloning

## Conclusion

**Performance loss**: 2-4x slower for creation/boot operations
**Resource overhead**: 512MB RAM per VM, but less disk space
**Critical gain**: Tailscale and VPN software now work

The trade-off is **worth it** for the improved compatibility and true isolation.
